{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播，找到参数最优\n",
    "# eg: loss = (w+1) 的平方\n",
    "# loss 对 w 求导得到 2*(w+1)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "lr = 0.2\n",
    "epoch = 40\n",
    "\n",
    "for epoch in range(epoch):  # for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。\n",
    "    with tf.GradientTape() as tape:  # with结构到grads框起了梯度的计算过程。\n",
    "        loss = tf.square(w + 1)\n",
    "    grads = tape.gradient(loss, w)  # .gradient函数告知谁对谁求导\n",
    "\n",
    "    w.assign_sub(lr * grads)  # .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads\n",
    "    print(\"After %s epoch,w is %f,loss is %f\" % (epoch, w.numpy(), loss))\n",
    "\n",
    "# lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程\n",
    "# 最终目的：找到 loss 最小 即 w = -1 的最优参数w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 创建一个tensor\n",
    "a = tf.constant([1,5], dtype=tf.int64)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 将numpy数据类型转换为tensor数据类型\n",
    "a = np.arange(0, 5)\n",
    "b = tf.convert_to_tensor(a, dtype=tf.int64)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 创建全为0的张量\n",
    "a = tf.zeros([2, 3])\n",
    "# 创建全为1的张量\n",
    "b = tf.ones(4)\n",
    "# 创建全为指定值的张量\n",
    "c = tf.fill([2, 2], 9)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 生成正态分布的随机数，默认均值为0，标准差为1\n",
    "# tf.random.normal(维度, mean=均值, stddev=标准差, dtype=数据类型, seed=随机种子, name=操作名称)\n",
    "a = tf.random.normal([2, 2], mean=0.5, stddev=1)\n",
    "print(\"a:\", a)\n",
    "# 生成截断式正态分布的随机数\n",
    "# tf.random.truncated_normal(维度, mean=均值, stddev=标准差, dtype=数据类型, seed=随机种子, name=操作名称)\n",
    "b = tf.random.truncated_normal([2, 2], mean=0.5, stddev=1)\n",
    "print(\"b:\", b)\n",
    "# 生成均匀分布随机数\n",
    "# tf.random.uniform(维度, minval=最小值, maxval=最大值, dtype=数据类型, seed=随机种子, name=操作名称)\n",
    "c = tf.random.uniform([2, 2], minval=0, maxval=1)\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1 = tf.constant([1., 2., 3.], dtype=tf.float64)\n",
    "print(\"x1:\", x1)\n",
    "# 强制tensor转换为该数据类型\n",
    "x2 = tf.cast(x1, tf.int32)\n",
    "print(\"x2\", x2)\n",
    "# 计算张量维度上元素的最小值\n",
    "print(\"minimum of x2：\", tf.reduce_min(x2))\n",
    "# 计算张量维度上元素的最大值\n",
    "print(\"maxmum of x2:\", tf.reduce_max(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([[1, 2, 3], [2, 2, 3]])\n",
    "print(\"x:\", x)\n",
    "# 计算张量沿着指定维度的平均值\n",
    "# tf.reduce_mean(张量名，axis=轴)\n",
    "print(\"mean of x:\", tf.reduce_mean(x))  # 求x中所有数的均值\n",
    "# 计算张量沿着指定维度的和\n",
    "# tf.reduce_sum(张量名，axis=轴)\n",
    "print(\"sum of x:\", tf.reduce_sum(x, axis=1))  # 求每一行的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.Variable()将变量标记为可训练，被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数\n",
    "# tf.Variable(初始值, dtype=数据类型, name=操作名称)\n",
    "w = tf.Variable(tf.random.normal([2,2]), mean=0.0, stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tensor中的数学运算\n",
    "a = tf.ones([1, 3])\n",
    "b = tf.fill([1, 3], 3.)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"a+b:\", tf.add(a, b))\n",
    "print(\"a-b:\", tf.subtract(a, b))\n",
    "print(\"a*b:\", tf.multiply(a, b))\n",
    "print(\"b/a:\", tf.divide(b, a))\n",
    "\n",
    "a = tf.fill([1, 2], 3.)\n",
    "print(\"a:\", a)\n",
    "print(\"a的n次方:\", tf.pow(a, 3))\n",
    "print(\"a的平方:\", tf.square(a))\n",
    "print(\"a的开方:\", tf.sqrt(a))\n",
    "\n",
    "# 矩阵的乘法\n",
    "a = tf.ones([3, 2])\n",
    "b = tf.fill([2, 3], 3.)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"a*b:\", tf.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 切分传入张量的第一维度,生成输入特征/标签对,构建数据集\n",
    "# data = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "features = tf.constant([12, 23, 10, 17])\n",
    "labels = tf.constant([0, 1, 1, 0])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# with结构记录计算过程,gradient求出张量的梯度\n",
    "# tape.gradient(函数, 对谁求导)\n",
    "with tf.GradientTape() as tape:\n",
    "    x = tf.Variable(tf.constant(3.0))\n",
    "    y = tf.pow(x, 2)\n",
    "grad = tape.gradient(y, x)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = ['one', 'two', 'three']\n",
    "for i, element in enumerate(seq):\n",
    "    print(i, element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# one_hot 函数将待转换数据,转换为one-hot形式的数据输出\n",
    "# tf.one_hot(待转换的数据, depth=几分类)\n",
    "classes = 3\n",
    "labels = tf.constant([1, 0, 2])  # 输入的元素值最小为0，最大为2\n",
    "output = tf.one_hot(labels, depth=classes)\n",
    "print(\"result of labels1:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# softmax使输出符合概率分布\n",
    "y = tf.constant([1.01, 2.01, -0.66])\n",
    "y_pro = tf.nn.softmax(y)\n",
    "\n",
    "print(\"After softmax, y_pro is:\", y_pro)  # y_pro 符合概率分布\n",
    "\n",
    "print(\"The sum of y_pro:\", tf.reduce_sum(y_pro))  # 通过softmax后，所有概率加起来和为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# assign_sub,赋值操作,更新参数的值并返回\n",
    "# 调用assign_sub前,先用tf.Variable定义变量w为可训练(可自更新)\n",
    "# x.assign_sub(x要自减的内容)\n",
    "x = tf.Variable(4)\n",
    "x.assign_sub(1)\n",
    "print(\"x:\", x)  # 4-1=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])\n",
    "print(\"test:\\n\", test)\n",
    "print(\"每一列的最大值的索引：\", tf.argmax(test, axis=0))  # 返回每一列最大值的索引\n",
    "print(\"每一行的最大值的索引\", tf.argmax(test, axis=1))  # 返回每一行最大值的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1 = tf.constant([[5.8, 4.0, 1.2, 0.2]])  # 5.8,4.0,1.2,0.2（0）\n",
    "w1 = tf.constant([[-0.8, -0.34, -1.4],\n",
    "                  [0.6, 1.3, 0.25],\n",
    "                  [0.5, 1.45, 0.9],\n",
    "                  [0.65, 0.7, -1.2]])\n",
    "b1 = tf.constant([2.52, -3.1, 5.62])\n",
    "y = tf.matmul(x1, w1) + b1\n",
    "print(\"x1.shape:\", x1.shape)\n",
    "print(\"w1.shape:\", w1.shape)\n",
    "print(\"b1.shape:\", b1.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "print(\"y:\", y)\n",
    "\n",
    "#####以下代码可将输出结果y转化为概率值#####\n",
    "y_dim = tf.squeeze(y)  # 去掉y中纬度1（观察y_dim与 y 效果对比）\n",
    "y_pro = tf.nn.softmax(y_dim)  # 使y_dim符合概率分布，输出为概率值了\n",
    "print(\"y_dim:\", y_dim)\n",
    "print(\"y_pro:\", y_pro)\n",
    "\n",
    "#请观察打印出的shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "x_data = datasets.load_iris().data  # .data返回iris数据集所有输入特征\n",
    "y_data = datasets.load_iris().target  # .target返回iris数据集所有标签\n",
    "# print(\"x_data from datasets: \\n\", x_data)\n",
    "# print(\"y_data from datasets: \\n\", y_data)\n",
    "\n",
    "x_data = DataFrame(x_data, columns=['花萼长度', '花萼宽度', '花瓣长度', '花瓣宽度']) # 为表格增加行索引（左侧）和列标签（上方）\n",
    "pd.set_option('display.unicode.east_asian_width', True)  # 设置列名对齐\n",
    "# print(\"x_data add index: \\n\", x_data)\n",
    "\n",
    "x_data['类别'] = y_data  # 新加一列，列标签为‘类别’，数据为y_data\n",
    "# print(\"x_data add a column: \\n\", x_data)\n",
    "\n",
    "#类型维度不确定时，建议用print函数打印出来确认效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.where(条件语句，真返回A， 假返回B)\n",
    "a = tf.constant([1, 2, 3, 1, 1])\n",
    "b = tf.constant([0, 1, 3, 4, 5])\n",
    "c = tf.where(tf.greater(a, b), a, b)  # 若a>b，返回a对应位置的元素，否则返回b对应位置的元素\n",
    "print(\"c：\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# np.random.RandomState.rand(维度) 返回一个[0, 1)之间的随机数，维度为空，返回标量\n",
    "rdm = np.random.RandomState(seed=1)\n",
    "a = rdm.rand()\n",
    "b = rdm.rand(2, 3)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# np.vstack 将两个数组按垂直方向叠加\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "c = np.vstack((a, b))\n",
    "print(\"c:\\n\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# np.mgrid[起始值:结束值:步长，起始值:结束值:步长]\n",
    "# x.ravel() 把x变为一维数组\n",
    "# np.c_[使返回的间隔数值点配对]\n",
    "# 生成等间隔数值点\n",
    "x, y = np.mgrid[1:3:1, 2:4:0.5]\n",
    "# 将x, y拉直，并合并配对为二维张量，生成二维坐标点\n",
    "grid = np.c_[x.ravel(), y.ravel()]\n",
    "print(\"x:\\n\", x)\n",
    "print(\"y:\\n\", y)\n",
    "print(\"x.ravel():\\n\", x.ravel())\n",
    "print(\"y.ravel():\\n\", y.ravel())\n",
    "print('grid:\\n', grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 交叉熵损失函数\n",
    "loss_ce1 = tf.losses.categorical_crossentropy([1, 0], [0.6, 0.4])\n",
    "loss_ce2 = tf.losses.categorical_crossentropy([1, 0], [0.8, 0.2])\n",
    "print(\"loss_ce1:\", loss_ce1)\n",
    "print(\"loss_ce2:\", loss_ce2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# softmax与交叉熵损失函数的结合\n",
    "# 输出先过softmax函数，再计算y与y_的交叉熵损失函数\n",
    "y_ = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]])\n",
    "y = np.array([[12, 3, 2], [3, 10, 1], [1, 2, 5], [4, 6.5, 1.2], [3, 6, 1]])\n",
    "\n",
    "y_pro = tf.nn.softmax(y)\n",
    "loss_ce1 = tf.losses.categorical_crossentropy(y_,y_pro)\n",
    "# 可以代替上面两行\n",
    "loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_, y)\n",
    "\n",
    "print('分步计算的结果:\\n', loss_ce1)\n",
    "print('结合计算的结果:\\n', loss_ce2)\n",
    "# 输出的结果相同"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
